---
tags: []
title: "10-bit é uma mentira"
date: 2019-10-30 03:51:43 +0000
excerpt: "Ou melhor, geralmente, muito do que é dito sobre 10-bit é uma mentira.  Não é a primeira vez que eu posto sobre isso. Aliás, acho que a..."
---

Ou melhor, geralmente, muito do que é dito sobre 10-bit é uma mentira.

Não é a primeira vez que eu posto sobre isso. Aliás, acho que a minha postagem anterior ficou até ruim porque misturei bit depth com chroma subsampling. Bem, no fim das coisas as duas coisas se misturam porque, embora 8-bit e 10-bit são definições distintas de bit depth os grupos confundem esses termos com profiles. Tentarei não cometer esse erro.

Mas por que eu estou postando novamente sobre isso? Primeiro, porque vi uma discussão sobre esse assunto em um chat de uma fansub onde uma imagem similar a esta¹ foi postada:

![](https://i.imgur.com/XUifVdX.png)

*Eu coloquei as aspas.* Considerando que vários grupos usam 10-bit é normal que quem os acompanhe tenha dúvidas sobre o que é isso e, nesse tipo de situação, é absurdamente comum usarem esse tipo de imagem para explicar. Alguns grupos também postam links para sites explicando o assunto, algumas vezes no contexto de vídeos, mas é muito mais frequente ver grupos postando explicações em outros contextos, como o de fotografias.

Vou explicar porque essa imagem está errada e é, ao mínimo, super-simplificação, mas antes, o segundo motivo para essa postagem:

Estou assistindo a segunda temporada daquele anime onde o protagonista pega garotas em uma Dungeon e em vários episódios estou notando o principal problema que o 10-bit deveria resolver: banding - em outras palavras - é possível ver faixas de cores no vídeo.

![](https://i.imgur.com/g95IG7u.png)

A imagem acima é de uma fansub que supostamente lança 10-bit para evitar justamente esse problema de banding, mesmo assim, como podemos ver, o problema não foi resolvido.

**Então...** por que 10-bit é geralmente uma mentira? Porque as razões apresentadas pelos grupos para usar essa configuração de bit depth são, na maioria dos casos, diferentes das razões reais desses grupos.

Voltando a primeira imagem, ela está errada por vários motivos. Em primeiro lugar, ambas são 8-bit. Estão exagerando os resultados para facilitar a compreensão? Que postem então um exemplo real.

Para começar comparar bit depth usando cores é complicado porque 8-bit e 10-bit se referem à quantidade de bits usados para cada cor em cada canal da imagem. Nesse caso é melhor trabalharmos com um único canal, ou seja, com uma imagem em preto e branco.

Infelizmente não posso postar uma imagem 10-bit real - até onde testei navegadores não suportam isso - mas vou simular usando dithering:

![](https://i.imgur.com/9G7KAEc.png)

Para melhor ilustrar a situação deixei em vermelho o que *deveria* acontecer se alguém copiar a imagem em um programa como o Paint, pegar a ferramenta "fill" (a do balde) e pintar um ponto. Experimente isso você mesmo.

Os gradientes dessa imagem tem 473 pixels de largura. Uma imagem cujas cores são definidas apenas por 8 bits conseguem ter uma variação de apenas 256 valores distintos, por tanto no gradiente abaixo algumas cores precisam se repetir, causando as faixas de cores. Se 10 bits forem usados, é possível ter uma variação de 1024 valores e assim as faixas são evitadas.

*Por esse motivo* é importante que alguém que trabalhe com imagens e vídeos use um bit depth maior quando possível: ter mais bits na imagem significa ter mais cores para trabalhar e assim minimizar o acúmulo de erros que pode ocorrer entre os vários processos envolvidos.

**Mas** isso não significa que 10-bit deve ser usado para distribuir o seu trabalho, por um motivo: compatibilidade. E não falo de compatibilidade com computadores velhos ou celulares e sim em geral:

No contexto de imagens não consegui encontrar um aplicativo sequer que conseguisse gerar ou mostrar uma imagem com um gradiente com 1024 de largura, de preto para branco, como o acima, sem banding. Testei o Photoshop, o ImageMagick, Nomacs, Chrome, Firefox.

No contexto de vídeos a situação é melhor: há players que conseguem simular 10-bit usando dithering e até há suporte para decodificação em hardware de H.265/HEVC e VP9 10-bit. Por outro lado esse hardware é relativamente novo: [nos processadores da Intel](https://software.intel.com/en-us/articles/enable-10bpp) essa tecnologia é suportada desde 2016 - logo nem o meu celular, que é de 2015, nem o meu computador, de 2011, suportam isso.

Se você não se importa com compatibilidade, que nem vários grupos fazem, o ideal seria usar AV1, HEVC ou VP9 10-bit, mas não H264, já que o suporte a H264 10-bit é bem menor e ele está ultrapassado de qualquer forma. Agora, se compatibilidade é um problema, use 8-bit: o meu celular não é ruim, ele roda HEVC Full HD - inclusive faz isso bem melhor do que o meu computador - mas não roda HEVC 10-bit.

Como praticamente todas as empresas que distribuem anime se preocupam com compatibilidade por não poderem dizer para seus clientes “baixe o MPV senão os arquivos não vão rodar direito”² elas distribuem anime em 8-bit.

Porque há grupos que insistem em usar 10-bit? Porque é mais fácil comprimir uma imagem com mais cores do que uma com menos cores, então os arquivos ficam menores. É difícil entender isso no início - afinal, se há mais cores logicamente o arquivo deveria ser maior, certo? - mas não é isso que acontece: com mais cores o encoder tem mais informações para processar a imagem e assim consegue trabalhar melhor.

Como as empresas distribuem os animes em 8-bit e os grupos geralmente não invadem os computadores das empresas de animação para conseguir as imagens originais eles tem que trabalhar com imagens em 8-bit. Como que eles convertem vídeos 8-bit para 10-bit? Olhe a imagem acima: ou não convertem, ou, se estão tentando, o trabalho deles é mal feito que nem dá para notar diferença.

Para concluir, sabem o que eu acho pior? Vários grupos usam 10-bit - que deveria representar que se preocupam com qualidade - mas deixam outras configurações de modo que a qualidade fica pior. Uma configuração que mais me incomoda é o chroma subsampling:

![](https://i.imgur.com/XswK3kR.png)

É possível notar que as transições de cores no 4:2:0 são mais borradas do que no 4:4:4, isso acontece porque os canais que armazenam as cores tem uma resolução *quatro vezes menor* no 4:2:0 do que no 4:4:4.

Isso é comum, absurdamente comum: em uma seleção de centenas de vídeos de vários grupos (bons e ruins, novos e antigos, rápidos e lentos), embora 48% deles é 10-bit, por outro lado 93% é 4:2:0 e **só** 6% é 4:4:4.

Esses grupos podem justificar que não usam porque, assim como na questão do bit depth, eles precisam simular uma qualidade maior, mas se usassem essa justificativa então não teriam como justificar o uso de 10-bit. Aposto que só não usam 4:4:4 por um motivo: porque isso não ajudar diminuir o tamanho do arquivo.

Em resumo:

* Grupos que dizem que 10-bit tem mais qualidade desconsideram o fato de que como as raws são 8-bit isso significa tirar qualidade de onde não existe.
* Grupos que usam 10-bit não se preocupam com compatibilidade porém com o tamanho dos arquivos (o que pode ser um indício de falta de qualidade).
* Caso algum grupo use 10-bit para reduzir o tamanho do arquivo, que não poste aquela imagem em uma tentativa falha de justificar qualidade.
* Caso algum grupo use 10-bit para melhorar a qualidade, que estude encoding a sério e não fique contendo lorota.
* Grupos que lançam H264 10-bit não sabem o que estão fazendo.
* Grupos que usam 10-bit e 4:2:0 sabem menos ainda.

¹ Não vou postar a imagem original tentando proteger a fansub. Meu objetivo não é falar mal desse grupo: eles fazem um bom trabalho... apesar do fato desse erro deles mostrar que não ou sabem muito sobre encoding ou não sabem explicar isso bem.

² A Crunchyroll parece ser uma exceção: se o player deles não funcionar no seu navegador eles te mandam baixar outro. ¯\_(ツ)_/¯
